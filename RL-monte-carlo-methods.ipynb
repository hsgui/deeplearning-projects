{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from basic_environments.gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridWorld Environment initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(3, 4)\n",
    "\n",
    "state_matrix = np.zeros((3, 4))\n",
    "state_matrix[0, 3] = 1 # terminal states\n",
    "state_matrix[1, 3] = 1 # terminal states\n",
    "state_matrix[1, 1] = -1 # obstacle\n",
    "\n",
    "reward_matrix = np.full((3, 4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "\n",
    "# For each one of the four actions there is a probability\n",
    "# This matrix defines the transition rules for all the 4 possible actions.\n",
    "# The first row corresponds to the probabilities of executing each one of\n",
    "# the 4 actions when the policy orders to the robot to go UP. In this case\n",
    "# the transition model says that with a probability of 0.8 the robot will\n",
    "# go UP, with a probaiblity of 0.1 RIGHT, 0.0 DOWN and 0.10 LEFT.\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])\n",
    "# Define the policy matrix\n",
    "# 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT, NaN=Obstacle, -1=NoAction\n",
    "# This is the optimal policy for world with reward=-0.04\n",
    "policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                          [0, np.NaN,  0,  -1],\n",
    "                          [0,      3,  3,   3]])\n",
    "# Set the matrices \n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -  -  -  * \n",
      " -  #  -  * \n",
      " ○  -  -  - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reset the environment\n",
    "observation = env.reset()\n",
    "#Display the world printing on terminal\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ACTION: 0.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " -  -  -  * \n",
      " ○  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 0.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " ○  -  -  * \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 1.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " -  ○  -  * \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 1.0\n",
      "REWARD: -0.04\n",
      "DONE: False\n",
      " -  -  ○  * \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n",
      "\n",
      "ACTION: 1.0\n",
      "REWARD: 1.0\n",
      "DONE: True\n",
      " -  -  -  ○ \n",
      " -  #  -  * \n",
      " -  -  -  - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(6):\n",
    "    action = policy_matrix[observation[0], observation[1]]\n",
    "    observation, reward, done = env.step(action)\n",
    "    print(\"\")\n",
    "    print(\"ACTION: \" + str(action))\n",
    "    print(\"REWARD: \" + str(reward))\n",
    "    print(\"DONE: \" + str(done))\n",
    "    env.render()\n",
    "    if done: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discount reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return(state_list, gamma):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        state_list: a list containing a tuple (position, reward)\n",
    "        gamma: discount factor\n",
    "    return:\n",
    "        a scalar value of total discounted return\n",
    "    \"\"\"\n",
    "    discount_factor = 1\n",
    "    total_return = 0\n",
    "    for visit in state_list:\n",
    "        immediate_reward = visit[1]\n",
    "        total_return += discount_factor * immediate_reward\n",
    "        discount_factor *= gamma\n",
    "        \n",
    "    return total_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC with known policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value matrix after 1 iterations:\n",
      "[[ 0.87712296  0.918041    0.959       1.        ]\n",
      " [ 0.83624584  0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "Value matrix after 10001 iterations:\n",
      "[[ 0.805301    0.86290038  0.91428591  1.        ]\n",
      " [ 0.75407064  0.          0.64573221 -1.        ]\n",
      " [ 0.69643872  0.6520033   0.          0.        ]]\n",
      "Value matrix after 20001 iterations:\n",
      "[[ 0.80787739  0.86524621  0.91642166  1.        ]\n",
      " [ 0.75674586  0.          0.65504244 -1.        ]\n",
      " [ 0.70355814  0.65563777  0.          0.        ]]\n",
      "Value matrix after 30001 iterations:\n",
      "[[ 0.80858277  0.86600025  0.91719579  1.        ]\n",
      " [ 0.75755278  0.          0.65622958 -1.        ]\n",
      " [ 0.70421566  0.65589214  0.          0.        ]]\n",
      "Value matrix after 40001 iterations:\n",
      "[[ 0.80810452  0.86559092  0.91681455  1.        ]\n",
      " [ 0.75707877  0.          0.65656877 -1.        ]\n",
      " [ 0.70245172  0.65710168  0.          0.        ]]\n",
      "Value matrix after 50000 iterations:\n",
      "[[ 0.80891482  0.86654924  0.91775168  1.        ]\n",
      " [ 0.75786165  0.          0.66140818 -1.        ]\n",
      " [ 0.70329865  0.657124    0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "value_matrix = np.zeros((3, 4))\n",
    "\n",
    "# init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full((3,4), 1.0e-10) \n",
    "\n",
    "gamma = 0.999\n",
    "total_epoch = 50000\n",
    "print_epoch = 10000\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    # starting a new episode\n",
    "    episode_list = list()\n",
    "    \n",
    "    # reset\n",
    "    observation = env.reset(exploring_starts = False)\n",
    "    for _ in range(1000):\n",
    "        # Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        # Move one step in the environment and get obs and reward\n",
    "        observation, reward, done = env.step(action)\n",
    "        # Append the visit in the episode list\n",
    "        episode_list.append((observation, reward))\n",
    "        if done: break\n",
    "            \n",
    "    # The episode is finished\n",
    "    counter = 0\n",
    "    # Checkup to identify if it is the first visit to a state\n",
    "    checkup_matrix = np.zeros((3,4))\n",
    "    # This cycle is the implementation of First-Visit MC.\n",
    "    # For each state stored in the episode list it checks if it\n",
    "    # is the first visit and then estimates the return.\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        row = observation[0]\n",
    "        col = observation[1]\n",
    "        reward = visit[1]\n",
    "        if(checkup_matrix[row, col] == 0): # first visit the state [row, col]\n",
    "            return_value = calculate_return(episode_list[counter:], gamma)\n",
    "            running_mean_matrix[row, col] += 1\n",
    "            value_matrix[row, col] += return_value\n",
    "            checkup_matrix[row, col] = 1\n",
    "        counter += 1\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"Value matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(value_matrix / running_mean_matrix)\n",
    "\n",
    "#Time to check the value matrix obtained\n",
    "print(\"Value matrix after \" + str(total_epoch) + \" iterations:\")\n",
    "print(value_matrix / running_mean_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility estimations for the states (4,1) and (3,1) are zero. This can be considered one of the limitations and at the same time one of the advantage of MC methods. \n",
    "\n",
    "The policy we are using, the transition probabilities, and the fact that **the robot always start from the same position (bottom-left corner) are responsible of the wrong estimation for those states**. \n",
    "\n",
    "Starting from the state (1,1) the robot will never reach those states and it cannot estimate the corresponding utilities. As I told you this is a problem because we cannot estimate those values but at the same time it is an advantage. In a very big grid world we can estimate the utilities only for the states we are interested in, saving time and resources and focusing only on a particular subspace of the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value matrix after 1 iterations:\n",
      "[[ 0.        0.918041  0.959     1.      ]\n",
      " [ 0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.      ]]\n",
      "Value matrix after 10001 iterations:\n",
      "[[ 0.80895536  0.86652174  0.91826584  1.        ]\n",
      " [ 0.75498824  0.          0.66168892 -1.        ]\n",
      " [ 0.6999732   0.64864294  0.61078822  0.32014546]]\n",
      "Value matrix after 20001 iterations:\n",
      "[[ 0.80795984  0.86578339  0.91728605  1.        ]\n",
      " [ 0.7552574   0.          0.66454441 -1.        ]\n",
      " [ 0.69752128  0.64614438  0.61276607  0.36230217]]\n",
      "Value matrix after 30001 iterations:\n",
      "[[ 0.80918353  0.86557823  0.9167413   1.        ]\n",
      " [ 0.75719981  0.          0.66114796 -1.        ]\n",
      " [ 0.69778092  0.64462955  0.60876963  0.38806528]]\n",
      "Value matrix after 40001 iterations:\n",
      "[[ 0.80810371  0.86504114  0.9164067   1.        ]\n",
      " [ 0.75639743  0.          0.65687153 -1.        ]\n",
      " [ 0.69748257  0.64470798  0.60605839  0.38750324]]\n",
      "Value matrix after 50000 iterations:\n",
      "[[ 0.80797203  0.8651374   0.91652576  1.        ]\n",
      " [ 0.75688371  0.          0.65880123 -1.        ]\n",
      " [ 0.69782815  0.64526183  0.60619295  0.39830374]]\n"
     ]
    }
   ],
   "source": [
    "value_matrix = np.zeros((3, 4))\n",
    "\n",
    "# init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full((3,4), 1.0e-10) \n",
    "\n",
    "gamma = 0.999\n",
    "total_epoch = 50000\n",
    "print_epoch = 10000\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    # starting a new episode\n",
    "    episode_list = list()\n",
    "    \n",
    "    # reset\n",
    "    observation = env.reset(exploring_starts = True)\n",
    "    for _ in range(1000):\n",
    "        # Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        # Move one step in the environment and get obs and reward\n",
    "        observation, reward, done = env.step(action)\n",
    "        # Append the visit in the episode list\n",
    "        episode_list.append((observation, reward))\n",
    "        if done: break\n",
    "            \n",
    "    # The episode is finished\n",
    "    counter = 0\n",
    "    # Checkup to identify if it is the first visit to a state\n",
    "    checkup_matrix = np.zeros((3,4))\n",
    "    # This cycle is the implementation of First-Visit MC.\n",
    "    # For each state stored in the episode list it checks if it\n",
    "    # is the first visit and then estimates the return.\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        row = observation[0]\n",
    "        col = observation[1]\n",
    "        reward = visit[1]\n",
    "        if(checkup_matrix[row, col] == 0): # first visit the state [row, col]\n",
    "            return_value = calculate_return(episode_list[counter:], gamma)\n",
    "            running_mean_matrix[row, col] += 1\n",
    "            value_matrix[row, col] += return_value\n",
    "            checkup_matrix[row, col] = 1\n",
    "        counter += 1\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"Value matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(value_matrix / running_mean_matrix)\n",
    "\n",
    "#Time to check the value matrix obtained\n",
    "print(\"Value matrix after \" + str(total_epoch) + \" iterations:\")\n",
    "print(value_matrix / running_mean_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the issue that not every state has value, we could explore the state randomly. \n",
    "\n",
    "To enable the exploring starts in our code the only thing to do is to set the parameter **exploring_strarts** in the **reset() function to True**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC without known policy -- Monte Carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_q_return(state_list, gamma):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        state_list: list of tuple (observation, action, reward)\n",
    "    \n",
    "    return q value of (observation, action)\n",
    "    \"\"\"\n",
    "    \n",
    "    total_q_return = 0.0\n",
    "    discount_factor = 1.0\n",
    "    for visit in state_list:\n",
    "        imm_reward = visit[2]\n",
    "        total_q_return += discount_factor * imm_reward\n",
    "        discount_factor *= gamma\n",
    "        \n",
    "    return total_q_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could imporve the performance\n",
    "def improve_policy(episode_list, policy_matrix, state_action_matrix):\n",
    "    \"\"\"\n",
    "    Imporve the policy by current new q values\n",
    "    return:\n",
    "        updated improved policy\n",
    "    \"\"\"\n",
    "    shape = policy_matrix.shape\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        col = observation[1] + (observation[0] * shape[1])\n",
    "        if policy_matrix[observation[0], observation[1]] >= 0:\n",
    "            policy_matrix[observation[0], observation[1]] = np.argmax(state_action_matrix[:, col])\n",
    "    return policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        policy: action: 0=Up, 1=right, 2=Down, 3=left, -1=terminal, -2=obstacle. Shape: (state_num, )\n",
    "    return: null\n",
    "    \"\"\"\n",
    "    \n",
    "    def action_translate(index):\n",
    "        if index == -1: return \" * \"\n",
    "        elif index == 0: return \" ^ \"\n",
    "        elif index == 1: return \" > \"\n",
    "        elif index == 2: return \" v \"\n",
    "        elif index == 3: return \" < \"\n",
    "        elif index == -2: return \" # \"\n",
    "        else: return \" - \"\n",
    "        \n",
    "    v_action_translate = np.vectorize(action_translate)\n",
    "    symbols = v_action_translate(policy)\n",
    "    print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 1 iterations:\n",
      "[[ 0.        0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.       -1.077961]\n",
      " [ 0.        0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.      ]\n",
      " [ 0.        0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.      ]]\n",
      "Policy matrix after 1 iterations:\n",
      "[[ 1  1  1 -1]\n",
      " [ 0 -2  3 -1]\n",
      " [ 2  3  3  0]]\n",
      "[[' > ' ' > ' ' > ' ' * ']\n",
      " [' ^ ' ' # ' ' < ' ' * ']\n",
      " [' v ' ' < ' ' < ' ' ^ ']]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-729480459e4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mnew_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mepisode_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\working-set\\git-repo\\deeplearning-projects\\basic_environments\\gridworld.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;31m#Based on the current action and the probability derived\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m#from the trasition model it chooses a new actio to perform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransition_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;31m#action = self.transition_model(action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shape = (3, 4)\n",
    "\n",
    "env = GridWorld(shape[0], shape[1])\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros(shape)\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "\n",
    "#Define the reward matrix\n",
    "reward_matrix = np.full(shape, -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "# Random policy matrix\n",
    "policy_matrix = np.random.randint(low=0, high=4, \n",
    "                                  size=shape)\n",
    "policy_matrix[1,1] = -2 # -2 for the obstacle at (1,1)\n",
    "policy_matrix[0,3] = policy_matrix[1,3] = -1 #No action (terminal states)\n",
    "\n",
    "#Set the matrices in the world\n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "# State-action matrix (init to zeros or to random values)\n",
    "state_action_matrix = np.zeros((4, shape[0] * shape[1])) # Q\n",
    "\n",
    "# init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full(state_action_matrix.shape, 1.0e-10) \n",
    "\n",
    "total_epoch = 100000\n",
    "print_epoch = 200000\n",
    "gamma = 0.999\n",
    "epsilon = 0.1\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    episode_list = list()\n",
    "    observation = env.reset(exploring_starts = True)\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        # Take the action from policy matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        if np.random.rand() < epsilon: # 10% random choose\n",
    "            action = np.random.randint(0, 4)\n",
    "            \n",
    "        new_observation, reward, done = env.step(action)\n",
    "        episode_list.append((observation, action, reward))\n",
    "        observation = new_observation\n",
    "        if done: break\n",
    "            \n",
    "    # The episode is finished.\n",
    "    counter = 0\n",
    "    # Checkup to identify if it is the first visit to a state-action\n",
    "    checkup_matrix = np.zeros_like(state_action_matrix)\n",
    "    # This cycle is the implementation of First-Visit MC.\n",
    "    # For each state-action stored in the episode list it checks if \n",
    "    # it is the first visit and then estimates the return. \n",
    "    # This is the Evaluation step of the GPI.\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        action = visit[1]\n",
    "        col = observation[1] + (observation[0]*shape[1])\n",
    "        row = action\n",
    "        if(checkup_matrix[row, col] == 0):\n",
    "            return_value = calculate_q_return(episode_list[counter:], gamma)\n",
    "            running_mean_matrix[row, col] += 1\n",
    "            state_action_matrix[row, col] += return_value\n",
    "            checkup_matrix[row, col] = 1\n",
    "        counter += 1\n",
    "    # Policy Update (Improvement)\n",
    "    policy_matrix = improve_policy(episode_list, \n",
    "                                   policy_matrix, \n",
    "                                   state_action_matrix/running_mean_matrix)\n",
    "    # Printing\n",
    "    if(epoch + 1 % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(state_action_matrix / running_mean_matrix)\n",
    "        print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(policy_matrix)\n",
    "        print_policy(policy_matrix)\n",
    "    \n",
    "# Time to check the utility matrix obtained\n",
    "print(\"Utility matrix after \" + str(total_epoch) + \" iterations:\")\n",
    "print(state_action_matrix / running_mean_matrix)\n",
    "print(policy_matrix)\n",
    "print_policy(policy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82247475  0.88623304  0.9494786   0.        ]\n",
      " [ 0.76526165  0.          0.68211216  0.        ]\n",
      " [ 0.70197114  0.64712202  0.59587756  0.34773295]]\n"
     ]
    }
   ],
   "source": [
    "state_action_value = state_action_matrix / running_mean_matrix\n",
    "optimal_value = np.argmax(state_action_value, axis = 0)\n",
    "optimal_value = state_action_value[optimal_value, range(len(optimal_value))]\n",
    "print(optimal_value.reshape(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
