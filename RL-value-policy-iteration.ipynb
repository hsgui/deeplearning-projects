{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html\n",
    "https://github.com/mpatacchiola/dissecting-reinforcement-learning/tree/master/src/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition matrix loaded from file\n",
    "# T[s, a, s'] = probability of transition from start state s, take action a, and enter state s'\n",
    "# for this issue, there are 3*4 = 12 states, 4 actions (left, right, up, down)\n",
    "T = np.load(\"resources\\T.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 12, 4)\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(T.shape)\n",
    "print(T[0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "T_tran = np.transpose(T, (0, 2, 1))\n",
    "print(T_tran.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([0.812, 0.868, 0.918,   1.0,\n",
    "                   0.762,   0.0, 0.660,  -1.0,\n",
    "                   0.705, 0.655, 0.611, 0.388])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.array([-0.04, -0.04, -0.04,   -0.04,\n",
    "                   -0.04,   -0.04, -0.04, -0.04,\n",
    "                   -0.04, -0.04, -0.04, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u.shape:  (12,) r.shape:  (12,)\n"
     ]
    }
   ],
   "source": [
    "print(\"u.shape: \", u.shape, \"r.shape: \", r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4)\n"
     ]
    }
   ],
   "source": [
    "u_temp = T_tran.dot(u)\n",
    "print(u_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "u_max = np.max(u_temp, axis=1)\n",
    "print(u_max.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8118  0.868   0.9178 -0.04  ]\n",
      " [ 0.762  -0.04    0.6604 -0.04  ]\n",
      " [ 0.7056  0.655   0.6111  1.4276]]\n"
     ]
    }
   ],
   "source": [
    "u_new = r + 1.0 * u_max\n",
    "u_shaped = u_new.reshape((3, 4))\n",
    "print(u_shaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action: 0=Up, 1=Left, 2=Down, 3=Right\n",
    "def state_value(current_value, T, reward, gamma):\n",
    "    \"\"\"\n",
    "    It's used in value iteration algorithm\n",
    "    params:\n",
    "        current_value: current value of each state. shape: (state_num, )\n",
    "        T: transition matrix. shape: (state_num, state_num, action_num)\n",
    "        reward: reward in each state. shape: (state_num, )\n",
    "        gamma: discount factor. 0 <= gamma <=1.0\n",
    "    \n",
    "    return:\n",
    "        new value of each state. shape: (state_num, ).\n",
    "        the policy. shape: (state_num, )\n",
    "    \"\"\"\n",
    "    \n",
    "    T_tran = np.transpose(T, (0, 2, 1)) # T_tran shape: (state_num, action_num, state_num)\n",
    "    expected_value = T_tran.dot(current_value) # shape: (state_num, action_num)\n",
    "    new_value = reward + gamma * np.max(expected_value, axis = 1)\n",
    "    policy = np.argmax(expected_value, axis = 1)\n",
    "    return (new_value, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
    "                   -0.04,   0.0, -0.04,  -1.0,\n",
    "                   -0.04, -0.04, -0.04, -0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy, shape):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        policy: action: 0=Up, 1=Left, 2=Down, 3=Right, -1=terminal, -2=obstacle. Shape: (state_num, )\n",
    "        shape: tuple. (row, column)\n",
    "    return: null\n",
    "    \"\"\"\n",
    "    \n",
    "    def action_translate(index):\n",
    "        if index == -1: return \" * \"\n",
    "        elif index == 0: return \" ^ \"\n",
    "        elif index == 1: return \" < \"\n",
    "        elif index == 2: return \" v \"\n",
    "        elif index == 3: return \" > \"\n",
    "        elif index == -2: return \" # \"\n",
    "        else: return \" - \"\n",
    "        \n",
    "    v_action_translate = np.vectorize(action_translate)\n",
    "    symbols = v_action_translate(policy)\n",
    "    print(symbols.reshape(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 26, delta=0.000010\n",
      "[[ 0.80796343  0.86539911  0.91653199  1.        ]\n",
      " [ 0.75696619  0.          0.65836281 -1.        ]\n",
      " [ 0.69968237  0.64881928  0.60471582  0.38149581]]\n",
      "Policy: \n",
      "[[' > ' ' > ' ' > ' ' * ']\n",
      " [' ^ ' ' # ' ' ^ ' ' * ']\n",
      " [' ^ ' ' < ' ' < ' ' < ']]\n"
     ]
    }
   ],
   "source": [
    "state_num = 12\n",
    "gamma = 0.999 # different gamma, the optimal policy and value is different\n",
    "iteration = 0\n",
    "epsilon = 0.01 # stopping criteria small value\n",
    "\n",
    "value_history = list()\n",
    "policy_history = list()\n",
    "value = np.zeros((state_num, ))\n",
    "for i in range(100):\n",
    "    delta = 0.0\n",
    "    iteration += 1\n",
    "    \n",
    "    new_value, new_policy = state_value(value, T, reward, gamma)\n",
    "    value_history.append(new_value)\n",
    "    policy_history.append(new_policy)\n",
    "    \n",
    "    delta = np.max(np.abs(new_value - value))\n",
    "    value = new_value\n",
    "\n",
    "    if delta < epsilon * (1-gamma) / gamma:\n",
    "        print(\"Iteration: %d, delta=%f\" % (iteration, delta))\n",
    "        print(value.reshape(3, 4))\n",
    "        print(\"Policy: \")\n",
    "        new_policy[5] = -2\n",
    "        new_policy[3] = new_policy[7] = -1\n",
    "        print_policy(new_policy, (3, 4))\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_policy(current_policy, current_value, T, reward, gamma):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        new value and new policy. shape: (state_num, )\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
